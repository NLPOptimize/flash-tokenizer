# Performance Test

This directory benchmarks accuracy and performance between the Python-bound FlashBertTokenizer and several other BertTokenizer implementations.

Performance and accuracy comparisons are conducted between FlashBertTokenizer and the following four tokenizer implementations via Python interfaces.

Tests can be run using various configurations and parquet files described in [DATA.md](../dataset/DATA.md). The ground truth for accuracy comparisons is the output generated by the original [BertTokenizer by Google](https://github.com/google-research/bert/blob/master/tokenization.py).

Accuracy is measured as the proportion of input_ids whose output token IDs exactly match the ground truth.

### Implementations compared:

* [Huggingface's BertTokenizerFast](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertTokenizerFast)
  * The most widely-used Rust implementation of BertTokenizerFast.

* [PaddleNLP's BertTokenizerFast](https://paddlenlp.readthedocs.io/en/stable/_modules/paddlenlp/experimental/faster_tokenizer.html)
  * A C++ implementation fully compatible with Huggingface's tokenizer, consistently about 1.2 times faster.

* [Tensorflow-text's FastBertTokenizer](https://www.tensorflow.org/text/api_docs/python/text/FastBertTokenizer)
  * An implementation provided by Tensorflow-text, but lacks notable advantages due to slower speed and lower accuracy.

* [Microsoft's BlingFire](https://github.com/microsoft/BlingFire)
  * A unique tokenizer developed by Microsoft, notable for having the fastest processing speed.

* [guillaume-be's rust-tokenizers](https://github.com/guillaume-be/rust-tokenizers)
  * Slower than BertTokenizerFlash and Blingfire but faster and more accurate than other implementations.

```bash
brew install rust
git clone https://github.com/guillaume-be/rust-tokenizers
cd rust-tokenizers/python-bindings
python setup.py install
```